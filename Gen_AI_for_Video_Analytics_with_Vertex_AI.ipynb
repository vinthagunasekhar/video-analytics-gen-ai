{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "use these commands to install the required packages."
      ],
      "metadata": {
        "id": "DqioYEDJZpE-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Iwsho9nJ-CI"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform\n",
        "!pip install langchain\n",
        "!pip install chromadb\n",
        "!pip install pytube\n",
        "!pip install youtube-transcript-api\n",
        "!pip install gradio\n",
        "from google.cloud import aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your account should have the Vertex AI user role."
      ],
      "metadata": {
        "id": "Clywf-eQaEkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "A1YqHuxmMDMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "PROJECT_ID = \"gunas1\" #enter your project id here\n",
        "vertexai.init(project=PROJECT_ID)"
      ],
      "metadata": {
        "id": "JpO2CYIUMYvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use these commands to import the required libraries."
      ],
      "metadata": {
        "id": "ybIwWAN2aSCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import YoutubeLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import VertexAI"
      ],
      "metadata": {
        "id": "LVb9_XUAMscJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the Vertex AI LLM model                                         \n",
        "Use this code snippet to initialize the Vertex AI LLM model. This initializes \"llm\" with the Text-Bison model of Vertex AI"
      ],
      "metadata": {
        "id": "9Klop0aKadZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = VertexAI(\n",
        "model_name=\"text-bison@001\",\n",
        "max_output_tokens=256,\n",
        "temperature=0.1, #The temperature is a parameter that controls the randomness of the LLM's output. A higher temperature will result in more creative and imaginative text, while a lower temperature will result in more accurate and factual text.\n",
        "top_p=0.8,\n",
        "top_k=40,\n",
        "verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "FRzjJp3hNRRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initiate Embeddings"
      ],
      "metadata": {
        "id": "rgZsWHGsaqYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "\n",
        "# Embedding\n",
        "EMBEDDING_QPM = 100 #Query per minute\n",
        "EMBEDDING_NUM_BATCH =5 #the batch output value\n",
        "embeddings = VertexAIEmbeddings(\n",
        "    requests_per_minute=EMBEDDING_QPM,\n",
        "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
        ")"
      ],
      "metadata": {
        "id": "6B52wV0KOAE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and chunk the video"
      ],
      "metadata": {
        "id": "pg76q9Qka0MU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = YoutubeLoader.from_youtube_url(\"https://youtu.be/8foQERR0mc0?si=iG0aU2D8ld3G89ZR\", add_video_info=True) #you can replace with any youtube link.\n",
        "result = loader.load()"
      ],
      "metadata": {
        "id": "pmGQFNSTOdi_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the video"
      ],
      "metadata": {
        "id": "pyRkyKgGa49z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0) #chunk size will tell each embedding has how many characters.\n",
        "docs = text_splitter.split_documents(result)\n",
        "print(f\"# of documents = {len(docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W-Yh51UOmCJ",
        "outputId": "9cd986a4-9074-47c5-d623-89e01d833fe9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of documents = 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store and retrieve\n",
        "\n",
        "Store your documents\n",
        "For this exercise, we are using ChromaDB. You can also use Vertex AI Vector Search. Store your documents and index them in ChromaDB as a vector store. ChromaDB is used to store and retrieve vector embeddings for use with LLMs and to perform semantic search over data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VqGP8320bA3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = Chroma.from_documents(docs, embeddings)\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "XZkycs1mPB2q"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a retriever chain                                       \n",
        "Create a retriever chain to answer the question. This is where we associate the Vertex AI Text Bison model LLM and the retriever that retrieves the embeddings from Chroma DB."
      ],
      "metadata": {
        "id": "4ndlDdWKbHKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type( llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
      ],
      "metadata": {
        "id": "BMCf_6MUPJK3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define your prompt"
      ],
      "metadata": {
        "id": "AlmLm7-bbTtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sm_ask(question, print_results=True):\n",
        "  video_subset = qa({\"query\": question})\n",
        "  context = video_subset\n",
        "  prompt = f\"\"\"\n",
        "  Answer the following question in a detailed manner, using information from the text below. If the answer is not in the text,say I dont know and do not generate your own response.\n",
        "\n",
        "  Question:\n",
        "  {question}\n",
        "  Text:\n",
        "  {context}\n",
        "\n",
        "  Question:\n",
        "  {question}\n",
        "\n",
        "  Answer:\n",
        "  \"\"\"\n",
        "  parameters = {\n",
        "  \"temperature\": 0.1,\n",
        "  \"max_output_tokens\": 256,\n",
        "  \"top_p\": 0.8,\n",
        "  \"top_k\": 40\n",
        "  }\n",
        "  response = llm.predict(prompt, **parameters)\n",
        "  return {\n",
        "  \"answer\": response\n",
        "\n",
        "  }"
      ],
      "metadata": {
        "id": "TzHjX2OXQ-ct"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrate the LLM Application                            \n",
        "Integrate the LLM application with Gradio for a visual front end interaction."
      ],
      "metadata": {
        "id": "fK7HJBPzbedZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "def get_response(input_text):\n",
        "  response = sm_ask(input_text)\n",
        "  return response\n",
        "\n",
        "grapp = gr.Interface(fn=get_response, inputs=\"text\", outputs=\"text\")\n",
        "grapp.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "4wUNletvRWqF",
        "outputId": "3e41894d-cdc8-4fec-872b-4c23896624ea"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://fabc20b5764af22d1b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fabc20b5764af22d1b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}